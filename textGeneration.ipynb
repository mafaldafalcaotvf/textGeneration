{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Source: https://github.com/hlamba28/Automatic-Image-Captioning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from numpy import array\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import string\n",
    "import os\n",
    "from PIL import Image\n",
    "import glob\n",
    "import pickle\n",
    "from pickle import dump, load\n",
    "from time import time\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Embedding, TimeDistributed, Dense, RepeatVector, \\\n",
    "                         Activation, Flatten, Reshape, concatenate, Dropout, BatchNormalization, Flatten\n",
    "from keras.optimizers import Adam, RMSprop\n",
    "from keras.layers.wrappers import Bidirectional\n",
    "from keras.layers.merge import add\n",
    "from keras.applications.inception_v3 import InceptionV3\n",
    "from keras.preprocessing import image\n",
    "from keras.models import Model\n",
    "from keras import Input, layers\n",
    "from keras import optimizers\n",
    "from keras.applications.inception_v3 import preprocess_input\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "dataDir = './data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done loading data_dict\n",
      "Done extracting objects from data_dict\n",
      "Done loading id_to_class and name_to_index\n",
      "Done splitting into train/validation\n"
     ]
    }
   ],
   "source": [
    "data_dict = pickle.load(open(os.path.join(dataDir, 'bird_data_dict.pkl'), 'rb'))\n",
    "print ('Done loading data_dict')\n",
    "\n",
    "\n",
    "X_sentence = np.array(data_dict['X_sentence'], dtype=np.int32) # (11788, 10, 76) numpy array of word IDS\n",
    "y_sentence = np.array(data_dict['y_sentence'], dtype=np.int32) # (11788, 10, 76) numpy array of next word IDS, or X[:,:,1:]\n",
    "lengths = np.array(data_dict['lengths'], dtype=np.int32) # (11788, 10) numpy array of lengths\n",
    "vocab_size = data_dict['num_words'] # no. words in vocab\n",
    "id_to_word = data_dict['id_to_word'] # dict: integer id to english word\n",
    "#print(idToWords(X_sentence[0], id_to_word))\n",
    "#print(idToWords(y_sentence[0], id_to_word))\n",
    "\n",
    "word_to_id = data_dict['word_to_id'] # dict: english word to id\n",
    "X_img = data_dict['X_img'] # (11788, 8192) numpy array of feature vectors\n",
    "y_img = data_dict['y_img'] # (11788, ) numpy array of classes\n",
    "max_length = data_dict['max_length'] # max length of sentence, or X_sentence.shape[-1]\n",
    "print ('Done extracting objects from data_dict')\n",
    "\n",
    "id_to_class = pickle.load(open(os.path.join(dataDir, 'id_to_class.pkl'), 'rb'))\n",
    "names = pickle.load(open(os.path.join(dataDir, 'names.pkl'), 'rb'))\n",
    "name_to_index = {} # dict: name of file to index in dataset\n",
    "for i, n in enumerate(names):\n",
    "    name_to_index[n] = i\n",
    "print ('Done loading id_to_class and name_to_index')\n",
    "\n",
    "# # ######## TRAIN/VAL SPLIT ##########\n",
    "with open(os.path.join(dataDir, 'train_no_cub.txt'), 'rb') as f:\n",
    "    train = [line.rstrip().decode('utf-8') for line in f.readlines()]\n",
    "    train_indices = [name_to_index.get(f) for f in train]\n",
    "with open(os.path.join(dataDir, 'val_no_cub.txt'), 'rb') as f:\n",
    "    val = [line.rstrip().decode('utf-8') for line in f.readlines()]\n",
    "    val_indices = [name_to_index.get(f) for f in val]\n",
    "\n",
    "train_val_overlap = bool(set(val_indices) & set(train_indices))\n",
    "assert not train_val_overlap\n",
    "\n",
    "X_img_train, y_img_train, X_sentence_train, y_sentence_train, lengths_train = X_img[train_indices, :], y_img[train_indices], X_sentence[train_indices], y_sentence[train_indices], lengths[train_indices]\n",
    "X_img_val, y_img_val, X_sentence_val, y_sentence_val, lengths_val = X_img[val_indices, :], y_img[val_indices], X_sentence[val_indices], y_sentence[val_indices], lengths[val_indices]\n",
    "print (\"Done splitting into train/validation\")\n",
    "\n",
    "# ######## Expand Descriptions ########\n",
    "# # 10 descriptions per image, flatten descriptions\n",
    "#X_sentence_train = X_sentence_train.reshape(X_sentence_train.shape[0]*X_sentence_train.shape[1], max_length)\n",
    "#X_sentence_val = X_sentence_val.reshape(X_sentence_val.shape[0]*X_sentence_val.shape[1], max_length)\n",
    "\n",
    "#y_sentence_train = y_sentence_train.reshape(y_sentence_train.shape[0]*y_sentence_train.shape[1], max_length)\n",
    "#y_sentence_val = y_sentence_val.reshape(y_sentence_val.shape[0]*y_sentence_val.shape[1], max_length)\n",
    "\n",
    "# lengths_train = lengths_train.flatten()\n",
    "# lengths_val = lengths_val.flatten()\n",
    "\n",
    "# # Repeat image 10 times to match description flattening\n",
    "#X_img_exp_train = np.tile(X_img_train, 10).reshape(X_img_train.shape[0]*10, X_img_train.shape[1])\n",
    "#X_img_exp_val = np.tile(X_img_val, 10).reshape(X_img_val.shape[0]*10, X_img_val.shape[1])\n",
    "#print (\"Done Expanding descriptions\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def buildingModel(vocabSize, maxLength):\n",
    "    inputLayer = Input(shape=(maxLength, vocabSize))\n",
    "    dropout = Dropout(0.5)(inputLayer)\n",
    "    lstm = LSTM(256, input_shape=(maxLength, vocabSize))(dropout)\n",
    "    denseLayer = Dense(256, activation='relu')(lstm)\n",
    "    outputLayer = Dense(vocabSize, activation='softmax')(denseLayer)\n",
    "    \n",
    "    return Model(inputs=inputLayer, outputs=outputLayer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4000, 10, 76)\n"
     ]
    }
   ],
   "source": [
    "print(X_sentence_train.shape)\n",
    "# print(X_img_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "`inSeq = pad_sequences([inSeq], maxlen=maxLength)[0]` [43, 12, 35, 67, 0, 0, 0, 0, 0, ..., 0]\n",
    "`inSeq = ...` [[0,0,0,1,0,0,0], 12, 35, 67, [0,0,0,0,0,0,0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "hotends = to_categorical([x for x in word_to_id.values()], vocab_size)\n",
    "id_to_hotend = dict(zip(word_to_id.values(), hotends))\n",
    "\n",
    "def hotend_to_id(hotend):\n",
    "    return np.argmax(hotend, axis=None, out=None)\n",
    "\n",
    "# Data generator, intended to be used in a call to model.fit_generator()\n",
    "def dataGenerator(descriptions, wordtoid, maxLength, vocabSize, batchSize=32):\n",
    "    # Incluir xMarkov quando implementar Markov Chain\n",
    "    xSentence, ySentence = list(), list()\n",
    "    n = 0\n",
    "    while 1:\n",
    "        for i, descriptions in zip(range(1, len(descriptions)), descriptions):\n",
    "            n += 1\n",
    "            for d in descriptions:\n",
    "                for i in range(1, len(d)):\n",
    "                    # split into input and output pair\n",
    "                    # maybe I could change this to => inSeq, outSeq = d[:i], dnext[:i]\n",
    "                    # where the dnext is extracted by the provided y_sentence\n",
    "                    inSeq, outSeq = d[:i], d[i]\n",
    "                    # pad input sequence\n",
    "                    # inSeq = pad_sequences([inSeq], maxlen=maxLength)[0]\n",
    "                    inSeq = np.array([id_to_hotend[x] for x in pad_sequences([inSeq], maxlen=max_length)[0]])\n",
    "                    # encode output sequence\n",
    "                    #outSeq = to_categorical([outSeq], num_classes=vocabSize)[0]\n",
    "                    outSeq = id_to_hotend[outSeq]\n",
    "                    # store\n",
    "                    xSentence.append(inSeq)\n",
    "                    ySentence.append(outSeq)\n",
    "            # yield the batch data\n",
    "            if n == batchSize:\n",
    "                yield [array(xSentence), array(ySentence)]\n",
    "                xSentence, ySentence = list(), list()\n",
    "                n = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_3 (InputLayer)         (None, 76, 6895)          0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 76, 6895)          0         \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, 256)               7323648   \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 6895)              1772015   \n",
      "=================================================================\n",
      "Total params: 9,161,455\n",
      "Trainable params: 9,161,455\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "lstm = buildingModel(vocab_size, max_length)\n",
    "lstm.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "lstm.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "callback = ModelCheckpoint(os.path.join(dataDir, 'models/weights.{epoch:02d}-{val_loss:.2f}.hdf5'), monitor='val_loss', save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "nEpochs = 10\n",
    "batchSize = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "trainGenerator = dataGenerator(X_sentence_train, word_to_id, max_length, vocab_size, batchSize)\n",
    "valGenerator = dataGenerator(X_sentence_val, word_to_id, max_length, vocab_size, batchSize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trainGenerator[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "  12/4000 [..............................] - ETA: 150349s - loss: 7.0995\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b"
     ]
    }
   ],
   "source": [
    "lstm.fit_generator(trainGenerator, steps_per_epoch = len(X_sentence_train)//batchSize, epochs=nEpochs, callbacks=[callback], validation_data=valGenerator, validation_steps=len(X_sentence_val)//batchSize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
